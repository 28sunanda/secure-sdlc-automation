#!/usr/bin/env python3
"""
Vulnerability Triage and Aggregation Script

This script aggregates security findings from multiple tools, deduplicates them,
assigns severity-based SLAs, and outputs a unified report.

Author: Sunanda Mandal
"""

import json
import os
import sys
import hashlib
import argparse
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional
import yaml


class VulnerabilityTriage:
    """
    Aggregates and triages security findings from multiple scanning tools.
    """
    
    # Severity normalization mapping
    SEVERITY_MAP = {
        # Semgrep
        'ERROR': 'CRITICAL',
        'WARNING': 'MEDIUM',
        'INFO': 'LOW',
        # Trivy/Grype
        'CRITICAL': 'CRITICAL',
        'HIGH': 'HIGH',
        'MEDIUM': 'MEDIUM',
        'LOW': 'LOW',
        'NEGLIGIBLE': 'INFO',
        # Bandit
        'HIGH': 'HIGH',
        'MEDIUM': 'MEDIUM',
        'LOW': 'LOW',
        # CVSS-based
        '9': 'CRITICAL',
        '8': 'HIGH',
        '7': 'HIGH',
        '6': 'MEDIUM',
        '5': 'MEDIUM',
        '4': 'MEDIUM',
        '3': 'LOW',
        '2': 'LOW',
        '1': 'LOW',
    }
    
    # Default SLA configuration (days to remediate)
    DEFAULT_SLA = {
        'CRITICAL': 1,    # 24 hours
        'HIGH': 7,        # 1 week
        'MEDIUM': 30,     # 1 month
        'LOW': 90,        # 3 months
        'INFO': None      # No SLA
    }
    
    def __init__(self, sla_config: Optional[str] = None):
        """
        Initialize the triage engine.
        
        Args:
            sla_config: Path to YAML file with custom SLA configuration
        """
        self.findings: List[Dict[str, Any]] = []
        self.metrics = {
            'total': 0,
            'critical': 0,
            'high': 0,
            'medium': 0,
            'low': 0,
            'info': 0,
            'new': 0,
            'duplicate': 0,
            'by_tool': {},
            'by_category': {}
        }
        
        # Load custom SLA if provided
        if sla_config and os.path.exists(sla_config):
            with open(sla_config, 'r') as f:
                self.sla = yaml.safe_load(f).get('sla', self.DEFAULT_SLA)
        else:
            self.sla = self.DEFAULT_SLA
    
    def generate_finding_hash(self, finding: Dict[str, Any]) -> str:
        """
        Generate a unique hash for deduplication.
        
        Uses: tool, rule_id, file, line (if available)
        """
        components = [
            finding.get('tool', ''),
            finding.get('rule_id', ''),
            finding.get('file', ''),
            str(finding.get('line', '')),
        ]
        hash_input = '|'.join(components)
        return hashlib.sha256(hash_input.encode()).hexdigest()[:16]
    
    def normalize_severity(self, severity: str) -> str:
        """Normalize severity to standard levels."""
        severity_upper = str(severity).upper()
        return self.SEVERITY_MAP.get(severity_upper, 'MEDIUM')
    
    def calculate_sla_deadline(self, severity: str) -> Optional[str]:
        """Calculate SLA deadline based on severity."""
        days = self.sla.get(severity)
        if days is None:
            return None
        deadline = datetime.now() + timedelta(days=days)
        return deadline.isoformat()
    
    def parse_semgrep(self, results_path: str) -> List[Dict[str, Any]]:
        """Parse Semgrep JSON output."""
        findings = []
        try:
            with open(results_path, 'r') as f:
                data = json.load(f)
            
            for result in data.get('results', []):
                finding = {
                    'tool': 'semgrep',
                    'rule_id': result.get('check_id', ''),
                    'title': result.get('extra', {}).get('message', ''),
                    'severity': self.normalize_severity(
                        result.get('extra', {}).get('severity', 'WARNING')
                    ),
                    'file': result.get('path', ''),
                    'line': result.get('start', {}).get('line', 0),
                    'code_snippet': result.get('extra', {}).get('lines', ''),
                    'cwe': result.get('extra', {}).get('metadata', {}).get('cwe', ''),
                    'owasp': result.get('extra', {}).get('metadata', {}).get('owasp', ''),
                    'remediation': result.get('extra', {}).get('metadata', {}).get('remediation', ''),
                    'raw': result
                }
                findings.append(finding)
        except Exception as e:
            print(f"Error parsing Semgrep results: {e}", file=sys.stderr)
        
        return findings
    
    def parse_bandit(self, results_path: str) -> List[Dict[str, Any]]:
        """Parse Bandit JSON output."""
        findings = []
        try:
            with open(results_path, 'r') as f:
                data = json.load(f)
            
            for result in data.get('results', []):
                finding = {
                    'tool': 'bandit',
                    'rule_id': result.get('test_id', ''),
                    'title': result.get('issue_text', ''),
                    'severity': self.normalize_severity(result.get('issue_severity', '')),
                    'confidence': result.get('issue_confidence', ''),
                    'file': result.get('filename', ''),
                    'line': result.get('line_number', 0),
                    'code_snippet': result.get('code', ''),
                    'cwe': result.get('issue_cwe', {}).get('id', ''),
                    'more_info': result.get('more_info', ''),
                    'raw': result
                }
                findings.append(finding)
        except Exception as e:
            print(f"Error parsing Bandit results: {e}", file=sys.stderr)
        
        return findings
    
    def parse_trivy(self, results_path: str) -> List[Dict[str, Any]]:
        """Parse Trivy JSON output."""
        findings = []
        try:
            with open(results_path, 'r') as f:
                data = json.load(f)
            
            for result in data.get('Results', []):
                target = result.get('Target', '')
                for vuln in result.get('Vulnerabilities', []):
                    finding = {
                        'tool': 'trivy',
                        'rule_id': vuln.get('VulnerabilityID', ''),
                        'title': vuln.get('Title', vuln.get('VulnerabilityID', '')),
                        'severity': self.normalize_severity(vuln.get('Severity', '')),
                        'file': target,
                        'package': vuln.get('PkgName', ''),
                        'installed_version': vuln.get('InstalledVersion', ''),
                        'fixed_version': vuln.get('FixedVersion', ''),
                        'description': vuln.get('Description', ''),
                        'cve': vuln.get('VulnerabilityID', ''),
                        'cvss': vuln.get('CVSS', {}),
                        'references': vuln.get('References', []),
                        'raw': vuln
                    }
                    findings.append(finding)
        except Exception as e:
            print(f"Error parsing Trivy results: {e}", file=sys.stderr)
        
        return findings
    
    def parse_safety(self, results_path: str) -> List[Dict[str, Any]]:
        """Parse Safety JSON output."""
        findings = []
        try:
            with open(results_path, 'r') as f:
                data = json.load(f)
            
            for vuln in data.get('vulnerabilities', []):
                finding = {
                    'tool': 'safety',
                    'rule_id': vuln.get('vulnerability_id', ''),
                    'title': vuln.get('advisory', ''),
                    'severity': self.normalize_severity(vuln.get('severity', 'MEDIUM')),
                    'package': vuln.get('package_name', ''),
                    'installed_version': vuln.get('analyzed_version', ''),
                    'vulnerable_versions': vuln.get('vulnerable_versions', ''),
                    'cve': vuln.get('CVE', ''),
                    'more_info': vuln.get('more_info_path', ''),
                    'raw': vuln
                }
                findings.append(finding)
        except Exception as e:
            print(f"Error parsing Safety results: {e}", file=sys.stderr)
        
        return findings
    
    def parse_gitleaks(self, results_path: str) -> List[Dict[str, Any]]:
        """Parse Gitleaks JSON output."""
        findings = []
        try:
            with open(results_path, 'r') as f:
                data = json.load(f)
            
            for result in data if isinstance(data, list) else []:
                finding = {
                    'tool': 'gitleaks',
                    'rule_id': result.get('RuleID', ''),
                    'title': f"Secret detected: {result.get('RuleID', '')}",
                    'severity': 'CRITICAL',  # Secrets are always critical
                    'file': result.get('File', ''),
                    'line': result.get('StartLine', 0),
                    'secret_type': result.get('RuleID', ''),
                    'commit': result.get('Commit', ''),
                    'author': result.get('Author', ''),
                    'date': result.get('Date', ''),
                    'raw': result
                }
                findings.append(finding)
        except Exception as e:
            print(f"Error parsing Gitleaks results: {e}", file=sys.stderr)
        
        return findings
    
    def parse_checkov(self, results_path: str) -> List[Dict[str, Any]]:
        """Parse Checkov JSON/SARIF output."""
        findings = []
        try:
            with open(results_path, 'r') as f:
                data = json.load(f)
            
            # Handle SARIF format
            if 'runs' in data:
                for run in data.get('runs', []):
                    for result in run.get('results', []):
                        finding = {
                            'tool': 'checkov',
                            'rule_id': result.get('ruleId', ''),
                            'title': result.get('message', {}).get('text', ''),
                            'severity': self.normalize_severity(
                                result.get('level', 'warning')
                            ),
                            'file': result.get('locations', [{}])[0].get(
                                'physicalLocation', {}
                            ).get('artifactLocation', {}).get('uri', ''),
                            'line': result.get('locations', [{}])[0].get(
                                'physicalLocation', {}
                            ).get('region', {}).get('startLine', 0),
                            'raw': result
                        }
                        findings.append(finding)
            else:
                # Handle native Checkov format
                for check_type in ['failed_checks', 'skipped_checks']:
                    for check in data.get('results', {}).get(check_type, []):
                        finding = {
                            'tool': 'checkov',
                            'rule_id': check.get('check_id', ''),
                            'title': check.get('check_name', ''),
                            'severity': 'HIGH' if check_type == 'failed_checks' else 'INFO',
                            'file': check.get('file_path', ''),
                            'resource': check.get('resource', ''),
                            'guideline': check.get('guideline', ''),
                            'raw': check
                        }
                        findings.append(finding)
        except Exception as e:
            print(f"Error parsing Checkov results: {e}", file=sys.stderr)
        
        return findings
    
    def aggregate_findings(self, input_dir: str) -> None:
        """
        Aggregate findings from all tools in the input directory.
        
        Args:
            input_dir: Directory containing scan result files
        """
        parsers = {
            'semgrep': self.parse_semgrep,
            'bandit': self.parse_bandit,
            'trivy': self.parse_trivy,
            'safety': self.parse_safety,
            'gitleaks': self.parse_gitleaks,
            'checkov': self.parse_checkov,
        }
        
        seen_hashes = set()
        
        for root, dirs, files in os.walk(input_dir):
            for file in files:
                if not file.endswith('.json'):
                    continue
                
                file_path = os.path.join(root, file)
                
                # Determine parser based on filename
                for tool_name, parser in parsers.items():
                    if tool_name in file.lower():
                        tool_findings = parser(file_path)
                        
                        for finding in tool_findings:
                            # Generate hash for deduplication
                            finding_hash = self.generate_finding_hash(finding)
                            
                            if finding_hash in seen_hashes:
                                self.metrics['duplicate'] += 1
                                continue
                            
                            seen_hashes.add(finding_hash)
                            
                            # Add metadata
                            finding['id'] = finding_hash
                            finding['timestamp'] = datetime.now().isoformat()
                            finding['sla_deadline'] = self.calculate_sla_deadline(
                                finding['severity']
                            )
                            finding['status'] = 'NEW'
                            
                            self.findings.append(finding)
                            
                            # Update metrics
                            self.metrics['total'] += 1
                            self.metrics['new'] += 1
                            severity_key = finding['severity'].lower()
                            if severity_key in self.metrics:
                                self.metrics[severity_key] += 1
                            
                            # Track by tool
                            tool = finding['tool']
                            self.metrics['by_tool'][tool] = \
                                self.metrics['by_tool'].get(tool, 0) + 1
                        
                        break
    
    def prioritize_findings(self) -> None:
        """Sort findings by severity and other factors."""
        severity_order = {'CRITICAL': 0, 'HIGH': 1, 'MEDIUM': 2, 'LOW': 3, 'INFO': 4}
        
        self.findings.sort(key=lambda x: (
            severity_order.get(x.get('severity', 'INFO'), 5),
            x.get('file', ''),
            x.get('line', 0)
        ))
    
    def generate_report(self) -> Dict[str, Any]:
        """Generate the final aggregated report."""
        self.prioritize_findings()
        
        return {
            'metadata': {
                'generated_at': datetime.now().isoformat(),
                'total_findings': self.metrics['total'],
                'sla_config': self.sla
            },
            'metrics': self.metrics,
            'findings': self.findings,
            'summary': {
                'blocking': self.metrics['critical'] + self.metrics['high'],
                'requires_attention': self.metrics['medium'],
                'informational': self.metrics['low'] + self.metrics.get('info', 0)
            }
        }
    
    def export_report(self, output_path: str) -> None:
        """Export the report to a JSON file."""
        report = self.generate_report()
        
        with open(output_path, 'w') as f:
            json.dump(report, f, indent=2, default=str)
        
        print(f"Report exported to: {output_path}")
        print(f"\n{'='*50}")
        print("SECURITY SCAN SUMMARY")
        print(f"{'='*50}")
        print(f"Total Findings: {self.metrics['total']}")
        print(f"  üî¥ Critical: {self.metrics['critical']}")
        print(f"  üü† High:     {self.metrics['high']}")
        print(f"  üü° Medium:   {self.metrics['medium']}")
        print(f"  üü¢ Low:      {self.metrics['low']}")
        print(f"\nDuplicates removed: {self.metrics['duplicate']}")
        print(f"\nFindings by tool:")
        for tool, count in self.metrics['by_tool'].items():
            print(f"  - {tool}: {count}")
        print(f"{'='*50}\n")


def main():
    parser = argparse.ArgumentParser(
        description='Aggregate and triage security findings from multiple tools'
    )
    parser.add_argument(
        '--input-dir', '-i',
        required=True,
        help='Directory containing scan result files'
    )
    parser.add_argument(
        '--output', '-o',
        default='aggregated-findings.json',
        help='Output file path'
    )
    parser.add_argument(
        '--sla-config', '-s',
        help='Path to SLA configuration YAML file'
    )
    
    args = parser.parse_args()
    
    triage = VulnerabilityTriage(sla_config=args.sla_config)
    triage.aggregate_findings(args.input_dir)
    triage.export_report(args.output)
    
    # Exit with non-zero if critical/high findings exist
    if triage.metrics['critical'] > 0 or triage.metrics['high'] > 0:
        print("‚ö†Ô∏è  BLOCKING FINDINGS DETECTED - Review required before merge")
        sys.exit(1)
    
    sys.exit(0)


if __name__ == '__main__':
    main()
